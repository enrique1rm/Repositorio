import pandas as pd
import Tarea3

from Tarea3 import df_interes

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# 1) Cargar y crear target
df = pd.read_csv("incident_event_log.csv")
df["opened_at"]  = pd.to_datetime(df["opened_at"], errors="coerce", dayfirst=True)
df["resolved_at"] = pd.to_datetime(df["resolved_at"], errors="coerce", dayfirst=True)
df["tiempo_resolver_horas"] = (df["resolved_at"] - df["opened_at"]).dt.total_seconds() / 3600

# 2) Features elegidas
cat_features = ["priority", "impact"]                # categóricas
num_features = ["reassignment_count", "reopen_count"]  # numéricas
features = cat_features + num_features

# 3) Filtrar filas con datos faltantes en las columnas que usaremos
df_model = df[features + ["tiempo_resolver_horas"]].dropna()
X = df_model[features]
y = df_model["tiempo_resolver_horas"]

# 4) Preprocesador
cat_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))   # si prefieres: OneHotEncoder(handle_unknown='ignore', drop='first')
])
preprocessor = ColumnTransformer(transformers=[
    ("cat", cat_transformer, cat_features),
    ("num", "passthrough", num_features)
])

# 5) Pipeline con Regresión Lineal
reg_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", LinearRegression())
])

# 6) Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# 7) Entrenar
reg_pipeline.fit(X_train, y_train)

# 8) Predecir y evaluar (RMSE calculado sin 'squared' param)
y_pred = reg_pipeline.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))    # <- aquí el cambio
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print("RMSE:", rmse)
print("MAE :", mae)
print("R2  :", r2)

# -------------------------
# 9) Extraer coeficientes y nombres de features
# -------------------------
# Obtener el encoder desde el preprocessor
pre = reg_pipeline.named_steps["preprocessor"]
ohe = pre.named_transformers_["cat"].named_steps["onehot"]

# Obtener nombres de categorías (robust: try get_feature_names_out, fallback a get_feature_names)
try:
    cat_names = ohe.get_feature_names_out(cat_features)   # sklearn >= 1.0
except AttributeError:
    # algunos sklearn antiguos usan get_feature_names()
    cat_names = ohe.get_feature_names(cat_features)

# Los nombres de features en orden: primero las columnas generadas por OHE, luego las numéricas (passthrough)
feature_names = list(cat_names) + list(num_features)

# Extraer coeficientes del regresor
coefs = reg_pipeline.named_steps["regressor"].coef_
intercept = reg_pipeline.named_steps["regressor"].intercept_

coef_df = pd.DataFrame({"feature": feature_names, "coef": coefs})
coef_df = coef_df.sort_values(by="coef", ascending=False).reset_index(drop=True)

print("\nIntercepto (β0):", intercept)
print("\nTop coeficientes (positivos -> aumentan horas):")
print(coef_df.head(12).to_string(index=False))
print("\nCoeficientes más negativos (reducción en horas):")
print(coef_df.tail(12).sort_values("coef").to_string(index=False))

